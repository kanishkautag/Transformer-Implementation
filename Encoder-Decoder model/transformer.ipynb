{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial positional encoding (pe) shape: torch.Size([10, 512])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Position tensor shape: torch.Size([10, 1])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "\n",
      "Div term shape: torch.Size([256])\n",
      "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
      "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
      "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
      "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
      "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
      "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
      "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
      "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
      "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
      "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
      "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
      "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7736e-02,\n",
      "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
      "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
      "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
      "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
      "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
      "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
      "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
      "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
      "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
      "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
      "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7736e-03, 7.4989e-03, 7.2339e-03,\n",
      "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
      "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
      "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
      "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
      "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
      "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
      "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
      "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
      "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
      "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
      "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
      "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
      "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
      "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
      "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
      "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
      "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
      "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
      "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
      "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])\n",
      "\n",
      "Updated positional encoding (pe) shape after sine and cosine:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
      "          1.0366e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
      "          2.0733e-04,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
      "          7.2564e-04,  1.0000e+00],\n",
      "        [ 9.8936e-01, -1.4550e-01,  9.9067e-01,  ...,  1.0000e+00,\n",
      "          8.2931e-04,  1.0000e+00],\n",
      "        [ 4.1212e-01, -9.1113e-01,  6.7637e-01,  ...,  1.0000e+00,\n",
      "          9.3297e-04,  1.0000e+00]])\n",
      "\n",
      "Positional encoding with batch dimension added (pe) shape: torch.Size([1, 10, 512])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
      "           1.0366e-04,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
      "           2.0733e-04,  1.0000e+00],\n",
      "         ...,\n",
      "         [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
      "           7.2564e-04,  1.0000e+00],\n",
      "         [ 9.8936e-01, -1.4550e-01,  9.9067e-01,  ...,  1.0000e+00,\n",
      "           8.2931e-04,  1.0000e+00],\n",
      "         [ 4.1212e-01, -9.1113e-01,  6.7637e-01,  ...,  1.0000e+00,\n",
      "           9.3297e-04,  1.0000e+00]]])\n",
      "\n",
      "Input tensor (x) shape: torch.Size([2, 10, 512])\n",
      "tensor([[[-1.0957,  1.2586,  0.6182,  ...,  0.8174, -1.4134, -0.2284],\n",
      "         [ 0.0096,  0.3625,  0.0740,  ..., -0.2106,  0.8374, -3.1795],\n",
      "         [ 0.0829,  1.2125, -1.1532,  ..., -0.2499,  0.3100,  0.8685],\n",
      "         ...,\n",
      "         [-0.0786,  0.4302, -1.8249,  ..., -0.5195,  0.9855, -1.3440],\n",
      "         [ 1.0190, -0.0709, -0.7206,  ...,  0.5736, -0.0298, -0.6947],\n",
      "         [ 1.3042, -1.1673, -0.6332,  ..., -0.2558,  0.2092,  1.4731]],\n",
      "\n",
      "        [[-0.5038,  0.0363, -0.8050,  ..., -0.7699, -0.2200, -0.8538],\n",
      "         [-0.1431, -1.0090,  0.6529,  ...,  0.5826, -0.5998,  0.7537],\n",
      "         [-0.1359, -0.1504,  0.2266,  ..., -0.4499,  0.9566, -0.0781],\n",
      "         ...,\n",
      "         [-1.9623,  0.6931,  1.4183,  ...,  0.3950, -1.2274,  0.4840],\n",
      "         [ 1.5039, -1.4181,  1.6714,  ..., -0.0543, -2.3477,  0.6691],\n",
      "         [-1.7331,  0.1158,  0.6364,  ...,  0.4834, -0.8078, -0.0671]]])\n",
      "\n",
      "Tensor after adding positional encoding:\n",
      "tensor([[[-1.0957,  2.2586,  0.6182,  ...,  1.8174, -1.4134,  0.7716],\n",
      "         [ 0.8511,  0.9028,  0.8959,  ...,  0.7894,  0.8375, -2.1795],\n",
      "         [ 0.9922,  0.7964, -0.2168,  ...,  0.7501,  0.3102,  1.8685],\n",
      "         ...,\n",
      "         [ 0.5784,  1.1841, -1.3725,  ...,  0.4805,  0.9862, -0.3440],\n",
      "         [ 2.0084, -0.2164,  0.2700,  ...,  1.5736, -0.0290,  0.3053],\n",
      "         [ 1.7164, -2.0785,  0.0432,  ...,  0.7442,  0.2102,  2.4731]],\n",
      "\n",
      "        [[-0.5038,  1.0363, -0.8050,  ...,  0.2301, -0.2200,  0.1462],\n",
      "         [ 0.6984, -0.4687,  1.4747,  ...,  1.5826, -0.5997,  1.7537],\n",
      "         [ 0.7734, -0.5665,  1.1630,  ...,  0.5501,  0.9568,  0.9219],\n",
      "         ...,\n",
      "         [-1.3053,  1.4470,  1.8707,  ...,  1.3950, -1.2267,  1.4840],\n",
      "         [ 2.4932, -1.5636,  2.6621,  ...,  0.9457, -2.3468,  1.6691],\n",
      "         [-1.3210, -0.7954,  1.3127,  ...,  1.4834, -0.8068,  0.9329]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "d_model = 512  # Example d_model\n",
    "seq_len = 10   # Example seq_len\n",
    "dropout_rate = 0.1  # Example dropout rate\n",
    "\n",
    "# Initialize the components\n",
    "dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "# Step 1: Create positional encoding matrix\n",
    "pe = torch.zeros(seq_len, d_model)\n",
    "print(\"Initial positional encoding (pe) shape:\", pe.shape)\n",
    "print(pe)\n",
    "\n",
    "# Step 2: Create position tensor\n",
    "position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
    "print(\"\\nPosition tensor shape:\", position.shape)\n",
    "print(position)\n",
    "\n",
    "# Step 3: Calculate div_term\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model / 2)\n",
    "print(\"\\nDiv term shape:\", div_term.shape)\n",
    "print(div_term)\n",
    "\n",
    "# Step 4: Apply sine and cosine\n",
    "pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "print(\"\\nUpdated positional encoding (pe) shape after sine and cosine:\")\n",
    "print(pe)\n",
    "\n",
    "# Step 5: Add batch dimension to the positional encoding\n",
    "pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "print(\"\\nPositional encoding with batch dimension added (pe) shape:\", pe.shape)\n",
    "print(pe)\n",
    "\n",
    "# Step 6: Simulate input tensor (x) for embeddings\n",
    "x = torch.randn(2, seq_len, d_model)  # Example input tensor with shape (batch_size, seq_len, d_model)\n",
    "print(\"\\nInput tensor (x) shape:\", x.shape)\n",
    "print(x)\n",
    "\n",
    "# Step 7: Add positional encoding (no gradient tracking)\n",
    "x = x + (pe[:, :x.shape[1], :]).requires_grad_(False)  # (batch, seq_len, d_model)\n",
    "print(\"\\nTensor after adding positional encoding:\")\n",
    "print(x)\n",
    "\n",
    "# Step 8: A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
